{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Note: you may have to add/clone/checkout some of these packages \n",
    "\n",
    "Notebook based on Tom Berloff works: http://www.breloff.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plots.GRBackend()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this re-exports Transformations, StochasticOptimization, Penalties, and ObjectiveFunctions\n",
    "using Learn\n",
    "\n",
    "# my version of ML iteration.  Hopefully will be replaced with what's currently in MLDataUtils dev branch\n",
    "using StochasticOptimization.Iteration\n",
    "\n",
    "import MLDataUtils: rescale!\n",
    "\n",
    "# for loading the data\n",
    "import MNIST\n",
    "\n",
    "# for plotting\n",
    "using StatPlots, MLPlots\n",
    "gr(leg=false, linealpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_test_loss (generic function with 2 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a one-hot matrix given class labels\n",
    "# TODO: this should be added as a utility in MLDataUtils\n",
    "function to_one_hot(y::AbstractVector)\n",
    "    yint = map(yi->round(Int,yi)+1, y)\n",
    "    nclasses = maximum(yint)\n",
    "    hot = zeros(Float64, nclasses, length(y))\n",
    "    for (i,yi) in enumerate(yint)\n",
    "        hot[yi,i] = 1.0\n",
    "    end\n",
    "    hot\n",
    "end\n",
    "\n",
    "# randomly pick a subset of testdata (size = totcount) and compute the total loss\n",
    "function my_test_loss(obj, testdata, totcount = 500)\n",
    "    totloss = 0.0\n",
    "    totcorrect = 0\n",
    "    for (x,y) in each_obs(rand(each_obs(testdata), totcount))\n",
    "        totloss += transform!(obj,y,x)\n",
    "\n",
    "        # logistic version:\n",
    "        # ŷ = output_value(obj.transformation)[1]\n",
    "        # correct = (ŷ > 0.5 && y > 0.5) || (ŷ <= 0.5 && y < 0.5)\n",
    "\n",
    "        # softmax version:\n",
    "        ŷ = output_value(obj.transformation)\n",
    "        chosen_idx = indmax(ŷ)\n",
    "        correct = y[chosen_idx] > 0\n",
    "\n",
    "        totcorrect += correct\n",
    "    end\n",
    "    totloss, totcorrect/totcount\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# our data:\n",
    "x_train, y_train = MNIST.traindata()\n",
    "x_test, y_test = MNIST.testdata()\n",
    "\n",
    "# normalize the input data given μ/σ for the input training data\n",
    "# note: scale both train and test sets using the train data\n",
    "μ, σ = rescale!(x_train)\n",
    "rescale!(x_test, μ, σ)\n",
    "\n",
    "# convert y data to one-hot\n",
    "y_train, y_test = map(to_one_hot, (y_train, y_test))\n",
    "\n",
    "# optional: limit to only 0/1 digits for easier training\n",
    "# to_isone(y::AbstractVector) = (z = Array(eltype(y), 1, length(y)); map!(yi->float(yi==1.0), z, y))\n",
    "# y_train, y_test = map(to_isone, (y_train, y_test))\n",
    "# train = filterobs(i -> y_train[i] < 1.5, x_train, y_train)\n",
    "# test = filterobs(i -> y_test[i] < 1.5, x_test, y_test)\n",
    "\n",
    "# store as tuples to make it easier\n",
    "train = (x_train, y_train)\n",
    "test = (x_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct our model and objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectiveFunctions.RegularizedObjective{Transformations.Chain{Float64,Transformations.Params{SubArray{Float64,1,Array{Float64,1},Tuple{UnitRange{Int64}},true},Tuple{},Tuple{}},Transformations.NoPreprocessing},ObjectiveFunctions.CrossEntropy{Float64},PenaltyFunctions.NoPenalty}(Chain{Float64}(\n",
       "   Affine{784-->50}\n",
       "   softplus{50}\n",
       "   Affine{50-->50}\n",
       "   softplus{50}\n",
       "   Affine{50-->10}\n",
       "   softmax{10}\n",
       ") ,ObjectiveFunctions.CrossEntropy{Float64}(10,Transformations.InputNode{:+,Float64,1}(Transformations.Node[Transformations.OutputNode{Float64,1}(Transformations.Node[Transformations.InputNode{:+,Float64,1}(#= circular reference @-4 =#)],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],Dict{Transformations.OutputNode{Float64,1},Int64}()),Transformations.InputNode{:+,Float64,1}(Transformations.Node[],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],Dict{Transformations.OutputNode{Float64,1},Int64}()),Transformations.OutputNode{Float64,1}(Transformations.Node[],[0.0],[1.0])),NoPenalty)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nin, nh, nout = 784, [50,50], 10\n",
    "using Learn\n",
    "# create a feedforward neural net with softplus activations and softmax output\n",
    "t = nnet(nin, nout, nh, :softplus, :softmax)\n",
    "\n",
    "# create an objective function with L2 penalty and an implicit cross entropy loss layer\n",
    "penalty = NoPenalty()\n",
    "obj = objective(t, penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain{Float64}(\n",
       "   Affine{784-->100}\n",
       "   softplus{100}\n",
       "   Affine{100-->100}\n",
       "   softplus{100}\n",
       "   Affine{100-->10}\n",
       "   softmax{10}\n",
       ") "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optional: set up plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "(totloss,accuracy) = (475.679555012944,0.12)\n"
     ]
    }
   ],
   "source": [
    "# the parts of the plot\n",
    "chainplt = ChainPlot(t, maxn=20)\n",
    "lossplt = TracePlot(title=\"Test Loss\", ylim=(0,Inf))\n",
    "accuracyplt = TracePlot(title=\"Accuracy\", ylim=(0.6,1))\n",
    "hmplt = heatmap(rand(28,28), ratio=1)\n",
    "\n",
    "# put together the full plot... a ChainPlot with loss, accuracy, and the heatmap\n",
    "plot(\n",
    "    chainplt.plt,\n",
    "    lossplt.plt,\n",
    "    accuracyplt.plt,\n",
    "    hmplt,\n",
    "    size = (1200,800),\n",
    "    layout=@layout([a; grid(1,3){0.2h}])\n",
    ")\n",
    "\n",
    "doanim = false\n",
    "# anim = Animation()\n",
    "\n",
    "# this is our custom callback which will be called on every 100 iterations\n",
    "# note: we do the plotting here.\n",
    "tracer = IterFunction((obj, i) -> begin\n",
    "    # sample points from the test set and compute/save the loss\n",
    "    @show i\n",
    "    if mod1(i,500)==500\n",
    "        totloss, accuracy = my_test_loss(obj, test, 200)\n",
    "        @show totloss, accuracy\n",
    "        push!(lossplt, i, totloss)\n",
    "        push!(accuracyplt, i, accuracy)\n",
    "    end\n",
    "\n",
    "    # add transformation data\n",
    "    update!(chainplt)\n",
    "\n",
    "    # update the heatmap of the total outgoing weight from each pixel\n",
    "    pixel_importance = reshape(sum(t[1].params.views[1],1), 28, 28)\n",
    "    # pixel_importance = reshape(abs(input_grad(t)),28,28)  # another possible metric\n",
    "    hmplt[1][1][:z].surf[:] = pixel_importance\n",
    "\n",
    "    # handle animation frames/output\n",
    "    if doanim\n",
    "        lastframe = 5000\n",
    "        if i < lastframe\n",
    "            frame(anim)\n",
    "        elseif i == lastframe\n",
    "            gif(anim, fps=10)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # display the plot\n",
    "    gui()\n",
    "end, every=100)\n",
    "\n",
    "# trace once before we start learning to see initial values\n",
    "tracer.f(obj, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a MetaLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StochasticOptimization.MetaLearner{Tuple{StochasticOptimization.GradientLearner{StochasticOptimization.FixedLR,StochasticOptimization.Adam{Float64},StochasticOptimization.GradientAverager},StochasticOptimization.IterFunction,StochasticOptimization.MaxIter}}((StochasticOptimization.GradientLearner{StochasticOptimization.FixedLR,StochasticOptimization.Adam{Float64},StochasticOptimization.GradientAverager}(StochasticOptimization.FixedLR(0.001),StochasticOptimization.Adam{Float64}(1.0e-8,0.9,0.999,#undef,#undef,#undef,#undef),StochasticOptimization.GradientAverager(#undef)),StochasticOptimization.IterFunction(#7,100),StochasticOptimization.MaxIter(1000)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = make_learner(\n",
    "    # averages the gradient over minibatches, updating params using the Adam method\n",
    "    GradientLearner(1e-3, Adam()),\n",
    "\n",
    "    # our custom iteration method\n",
    "    tracer,\n",
    "\n",
    "    # shorthand to add a MaxIter(10000)\n",
    "    maxiter = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 100\n",
      "i = 200\n",
      "i = 300\n",
      "i = 400\n",
      "i = 500\n",
      "(totloss,accuracy) = (81.96275618777746,0.88)\n",
      "i = 600\n",
      "i = 700\n",
      "i = 800\n",
      "i = 900\n",
      "i = 1000\n",
      "(totloss,accuracy) = (63.086946687790714,0.895)\n"
     ]
    }
   ],
   "source": [
    "# do the learning... average over minibatches of size 5 for maxiter iterations\n",
    "learn!(obj, learner, infinite_batches(train, size=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save an image of the training output\n",
    "png(\"/tmp/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: anim not defined\nwhile loading In[22], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: anim not defined\nwhile loading In[22], in expression starting on line 1",
      ""
     ]
    }
   ],
   "source": [
    "mp4(anim, fps=10);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
